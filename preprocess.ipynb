{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing code\n",
    "\n",
    "This is meant to be a notebook for quickly converting CSV / etc into JSON using Python. This code is not annotated as it is more or less just preprocessing code fragments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "from random import choices\n",
    "import re\n",
    "\n",
    "# berlin_covid = pd.read_csv(\"static/covid_data_dssg_project.csv\")\n",
    "# bike_data = pd.read_csv(\"static/fetched_eco_bike_data-overall_germany.csv\")\n",
    "# json_data = json.load( open(\"static/berlin_riders.json\") )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert `Berlin_x.csv` files into JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "for i in range(1,18+1):\n",
    "    filename = f\"unused/Berlin_{i}.csv\"\n",
    "    temp = pd.read_csv(filename).drop(columns=[\"Unnamed: 0\", \"timestamp\"])\n",
    "    temp[\"date\"] = temp[\"date\"].apply(lambda x: x[:10])\n",
    "    grouped = temp.groupby(\"date\").sum(\"comptage\")\n",
    "    grouped[\"group\"] = f\"Berlin_{i}\"\n",
    "    dataframes.append(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final = pd.concat(dataframes)\n",
    "final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaljson = {}\n",
    "x = 0\n",
    "for index, rows in final.iterrows():\n",
    "    if index not in finaljson: finaljson[index] = {} \n",
    "    finaljson[index][rows[\"group\"]] = rows[\"comptage\"] # {\"group\": rows[\"group\"], \"comptage\": rows[\"comptage\"]}\n",
    "    if x > 10: break\n",
    "    \n",
    "json.dump(finaljson, open(\"static/berlin_big_table.json\", \"w\"))\n",
    "# print(finaljson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j = json.load(open(\"static/data_daily.json\"))\n",
    "# the_json = json.loads(j)\n",
    "# # the_json[\"2016-01-01 00:00:00\"]\n",
    "\n",
    "# new_json = {}\n",
    "# for rows in the_json:\n",
    "#     new_date = rows.split(\" \")[0] # this might change depending on the new data\n",
    "#     new_json[new_date] = {}\n",
    "#     for entry in the_json[rows]:\n",
    "#         new_json[new_date][entry[\"group\"]] = entry[\"comptage\"]\n",
    "    \n",
    "# new_json\n",
    "\n",
    "# json.dump(new_json, open(f\"static/daily_json_processed-{choices('wertyuio', k=15)}.json\", \"w\")) # turn this on when you want to write the data!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# berlin_covid_indexed = berlin_covid.set_index(\"date\")\n",
    "\n",
    "# # data = {}\n",
    "# for row in berlin_covid.iterrows():\n",
    "#     print(row)\n",
    "#     break\n",
    "# berlin_json = berlin_covid_indexed.to_json(orient=\"index\")\n",
    "\n",
    "# json.dump(berlin_json, open(\"static/berlin_covid.json\", \"w\"))\n",
    "\n",
    "# from pprint import pprint\n",
    "# pprint(berlin_covid_indexed)\n",
    "# berlin_covid_indexed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newjson = {}\n",
    "\n",
    "for key in json_data:\n",
    "    # print(key)\n",
    "    newdate = datetime.datetime.fromtimestamp(\n",
    "        int(key)/1000).strftime('%Y-%m-%d')\n",
    "    # print(newdate.strftime('%Y-%m-%d'))\n",
    "\n",
    "    newjson[newdate] = []\n",
    "\n",
    "    for day in json_data[key]:\n",
    "        newgroup = day[\"group\"].lstrip(\"ADFC\\/\").rstrip(\".csv\").replace(\"_\", \"\")\n",
    "        newjson[newdate].append(\n",
    "            {\"comptage\": day[\"comptage\"], \"group\": newgroup})\n",
    "\n",
    "json.dump(newjson, open(\"static/berlin_riders_processed.json\", \"w\"))\n",
    "\n",
    "newjson\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40e2eb60aa1a11f2fe87c70e9a895dc0d64f6821c8fd8cbe344969fe515db466"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
